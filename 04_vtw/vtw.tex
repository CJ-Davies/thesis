\begin{quote}
	\textit{``The sinister thing about a simstim construct, really, was that it carried the suggestion that any environment might be unreal, that the windows of the shopfronts she passed now with Andrea might be figments.''}
\end{quote}
\hfill \textit{Count Zero, William Gibson}
\\
\\
\\

%=========================================================================================================
%=========================================================================================================

This chapter presents the development of a preliminary PR system that combined a tablet computer, GPS, accelerometer \& magnetometer, with an OpenSim based virtual environment to allow exploration of the real world ruins of a 14th century cathedral with a virtual reconstruction of it as it stood in its prime. Cultural heritage is introduced as an ideal area for which PR systems can be applied to beneficial effect, while the accuracy of GPS tracking emerged as a constraint on this style of implementation.

%=========================================================================================================

\section{Virtual Heritage}

Alternate reality technologies have been used for over two decades~\cite{Roussou2002} to aid in the investigation, understanding \& dissemination of information pertaining to our past in the fields of archaeology \& cultural heritage. Whilst archaeology studies human activity through the recovery of remains, heritage is also concerned with intangible attributes of society; tradition, art, narratives \& other cultural evidences~\cite{Roussou2002}. \textit{Virtual heritage} is the name given to the application of advanced imaging techniques, including alternate reality techniques, to the synthesis, conservation, reproduction, representation, reprocessing \& display of this cultural evidence~\cite{roussou:photorealism}.

These techniques provide access to locations \& artefacts scattered about the world, that may reside in private collections inaccessible to scholars (much less interested amateurs) \& outwith of their original context of creation~\cite{griffin:recovering}. They allow recreations to be made of the numerous cultural heritage objects that are deteriorating or are at risk of being lost, both due to natural causes such as weather \& natural disasters but also due to acts of man such as civil war~\cite{Ikeuchi2003}.

Virtual heritage techniques offer substantial benefits to collaborative investigation of sites, where multiple users are provided the ability to collaborate via a multitude of different visualization modalities including video see-through head-tracked head mounted displays, projected table surfaces, large screen displays \& tracked hand-held displays, including the ability for experts physically located at a particular site to collaborate with those remote to it~\cite{benko:collaborative}. This combination of different techniques not only benefits experts, but has been used in the creation of contiguous platforms for building \& managing exhibitions of 3D models of artefacts accessed in museums, galleries \& via the Web~\cite{Wojciechowski2004}, focussed not only on the digitization \& subsequent interaction with such content to aid in its preservation \& protection, but also with making these resources as widely available as possible to any interested parties (scientists, archaeologists, curators, historians \& interested amateurs)~\cite{walczak:applications}.

Even traditionally two-dimensional visual resources associated with cultural heritage can be integrated into such state-of-the-art systems, visualized via immersive CAVE techniques as part of `information landscapes'~\cite{Ruffaldi2008}. Such techniques are of particular benefit to young people as cultural heritage sites often arouse little involvement in them, especially if the site's present day appearance bears few traces of its original stature \& makes it difficult to appreciate its original splendour \& importance~\cite{ardito:combining}.

%=========================================================================================================

\subsection{Alternate Reality Techniques in Virtual Heritage}

Due to the number of combinations \& diversity in approaches that have been used in the application of visualization techniques to the cultural heritage sector, attempting to comprehensively list them is impractical. Comparison via a taxonomical model that classifies approaches according to various characteristics is thus the approach adopted by Foni et al.~\cite{Foni2010} that produced the taxonomical space shown by figure \ref{taxonomy_1.png}.

\begin{figure}[h]
\centering
  \includegraphics[width=.7\linewidth]{taxonomy_1.png}
  \caption{Taxonomical space of visualization strategies used in cultural heritage.}
  \label{taxonomy_1.png}
\end{figure}

This model classifies visualization strategies according to four continua, represented by the three physical dimensions of the 3D cube \& the degree of shading of each point within the cube. The x axis represents the level of automatism, which refers to the span of the development cycle required to produce the visualization; the y axis represents the level of precision, referring not only to the amount of geometrical detail but to all elements that can contribute to or enhance reliability; \& the z axis represents the level of interactivity, defined in this context as;

\begin{quote}
	\textit{``its capacity to contextually offer the possibility to subjectively experience an interactive behaviour in a synchronous way, thus enabling the user the opportunity to meaningfully contribute to a given experience or to affect in real time the visualized  item''}~\cite{Foni2010}.
\end{quote}

The shading of each point within the cube represents its degree of virtuality, conceptually analogous to the reality-virtuality continuum of Milgram et al. (see section \ref{milgram&kishino}) with real world/world unmodelled represented as solid black, virtual reality/world completely modelled as completely white \& positions in-between as various shades of grey. The position of 16 exemplar visualization techniques applied to cultural heritage are shown upon the cube \& explained via the table figure \ref{taxonomy_2.png}, which includes both traditional techniques \& state-of-the-art methodologies.

\begin{figure}[h]
\centering
  \includegraphics[width=.8\linewidth]{taxonomy_2.png}
  \caption{Coordinate sets for each approach within the taxonomical space (figure \ref{taxonomy_1.png})}
  \label{taxonomy_2.png}
\end{figure}

%=========================================================================================================

In terms of alternate reality techniques, real time AR simulations (category 13) have been used to add artefacts, actors \& reconstructed architecture to views of present day sites that are still accessible \& may bear traces of their original status, whilst real time VR simulations (category 10) have been used to host more complete reconstructions of entire buildings \& settlements for interaction via screen, HMD \& CAVE, including where the present day site bears no evidence of its past status or is inaccessible due to latter development, change in landscape, etc..

%=========================================================================================================

The ARCHEOGUIDE project (Augmented Reality-based Cultural Heritage On-site GUIDE)~\cite{vlahakis:archeoguide} aimed to provide a `personalized electronic guide \& tour assistant' to cultural heritage site visitors. On-site help \& augmented reality reconstructions of on-site ruins were presented via a laptop, a tablet computer \& a PDA, using GPS for location tracking \& magnetometer to ascertain direction such that augmentations could be placed accordingly. The applications claimed to be supported by the platform range from archaeological research to education, multimedia publishing \& cultural tourism. The platform was prototyped at the archaeological site of Olympia, Greece.

As well as being used for walking tours, AR has been combined with the concept of telepresence to create `augmented telepresence', allowing participants to experience a `fly-through' of the ancient Nara Heijo-kyo capital of Japan, by combining aerially captured omnidirectional video augmented with related information using AR techniques~\cite{Okura2006,Okura2011}.

Augmenting views of the real world with real-time animated virtual humans has been explored by several projects, including the LIFEPLUS EU IST project, which aimed to produce `\textit{`an innovative 3D reconstruction of ancient frescos-paintings through the real-time revival of their fauna and flora, featuring groups of virtual animated characters with artificial life dramaturgical behaviors, in an immersive AR environment''}. This project pushed established augmented reality applications to the field by exploring narrative design in fictional spaces, with the aim of increasing immersion via realistic interaction, making use of captured/real-time video of a real scene~\cite{Papagiannakis2004}, presenting the visitor with \textit{``an immersive and innovative multi-sensory interactive trip to the past''}~\cite{Papagiannakis2005}. These realistic simulations of animated virtual human actors were employed in a mobile \& wearable setup, in abandonment of traditional concepts of static cultural artefacts or rigid augmentations of real world features, making use of a markerless camera tracker \& mixed reality illumination model for more consistent real-virtual \& virtual-real rendering. This platform was demonstrated in a case study on the real site of ancient Pompeii \& whilst initially targeted at the cultural heritage sector, the author(s) clarifies that as a platform it is not limited to such subjects~\cite{Papagiannakis2007}. This concept of extending rigid \& static AR with character-based event representations hopes to recreate not just discrete artefacts but the entirety of `daily life' at the scene~\cite{Papagiannakis2009}.

Although many applications of AR to cultural heritage sites are mobile in nature, using a variety of tracking techniques to localise the user \& determine their orientation, including GPS~\cite{vlahakis:archeoguide}, visual tracking of robust features of the environment~\cite{Kim2009} \& omnidirectional range sensing of a landmark database~\cite{Taketomi2011}, there are also those that present a static interface similar to coin-operated telescopes situated at popular tourist attractions~\cite{Weng2012}.

%=========================================================================================================

VR is not only useful where the real site is no longer accessible, too remote or does not bear any similarity to its original status, but also allows for more effective control over the atmospheric qualities of the environment being recreated; effects such as fog, sky, water \& particles, exploiting the latest graphical hardware by making use of shaders to deliver high quality graphics~\cite{deamicis:gamebased}. The use of a head mounted display or CAVE~\cite{cabral:x3dexperience,Christou2006} that completely blocks stimuli from the user's real world surroundings allows for this complete level of control. Unless an AR system employs various environmental monitoring techniques, the augmentations that it overlays upon the user's view of the real world will often have differing illumination than their real surroundings which has an effect upon their perceived realism~\cite{mcnamara:lightness}.

Whereas many heritage representations, architectural walkthroughs \& simulations of artefacts \& places have defined a practice where photorealism is considered an important measure of the representation's success, there is an argument that whilst such an emphasis on realism \& historical accuracy \& authenticity is important, such photorealistic methods can limit the flexibility of the reconstructions with regards to how much they can be modified \& altered to explore different reconstruction hypotheses~\cite{roussou:photorealism}. Emphasis on photorealistic graphical quality also has considerations when it comes to real time performance \& many intelligent techniques must be employed to maintain acceptable performance as complexity of reconstructions increases~\cite{willmott:largecomplex}. Particularly for dissemination to the public in museums \& visitor centres, acceptable performance is often more important than extreme historical accuracy.

%=========================================================================================================

\subsection{Virtual Heritage at the University of St Andrews}

\textbf{***Include references to Kris Getchell's thesis? Experiential learning, etc?}

The Open Virtual Worlds (OVW) research group at the School of Computer Science at the University of St Andrews has been employed in virtual heritage projects since 2007~\cite{Getchell2007}, producing a number of reconstructions of cultural heritage sites in Scotland \& further afield. These reconstructions have been produced through collaborations with academics from the university's Art History, History \& Archaeology departments, as well as with domain experts from heritage organisations including Historic Scotland \& the National Trust for Scotland. These projects range from small reconstructions of a church to much larger reconstructions such as that of the cathedral at St Andrews (figure \ref{cathedral_real_outside.jpg}, \ref{cathedral_reconstruction_above.jpg}) which represents several years of work~\cite{Kennedy2013}. Whilst the cathedral reconstruction was completed as a research project, other reconstructions were produced specifically for use in schools in Scotland (such as Linlithgow palace, figure \ref{linlithgow_real.jpg}, \ref{linlithgow_reconstruction.png}), others for outreach purposes (Mosfell Viking farmstead, figure \ref{mosfell_outside.jpg}, \ref{mosfell_inside.jpg}) \& still others were built specifically for installation into museums (Caen Township, figure \ref{caen_township_outside.jpg}, \ref{caen_township_inside_wireframe.jpg}). Some of these reconstructions are inhabited with virtual humans that are scripted to perform certain actions specific to the role they depict at the site (figure \ref{cathedral_npc_standing.png}, \ref{cathedral_npc_talking.png}).

\TwoFig{cathedral_real_outside.jpg} {St Andrews cathedral today.} {cathedral_real_outside.jpg}
       {cathedral_reconstruction_above.jpg} {St Andrews cathedral reconstruction.} {cathedral_reconstruction_above.jpg}

\TwoFig{linlithgow_real.jpg} {Linlithgow Palace today.} {linlithgow_real.jpg}
       {linlithgow_reconstruction.png} {Linlithgow Palace reconstruction.} {linlithgow_reconstruction.png}

\TwoFig{mosfell_outside.jpg} {Mosfell Viking Longhouse.} {mosfell_outside.jpg}
	   {mosfell_inside.jpg} {Longhouse interior.} {mosfell_inside.jpg}

\TwoFig{caen_township_outside.jpg} {Caen Township.} {caen_township_outside.jpg}
       {caen_township_inside_wireframe.jpg} {Caen Township wireframe detail.} {caen_township_inside_wireframe.jpg}

\TwoFig{cathedral_npc_standing.png} {Virtual humans in cathedral reconstruction.} {cathedral_npc_standing.png}
       {cathedral_npc_talking.png} {Conversing with virtual humans.} {cathedral_npc_talking.png}

These reconstructions were made using OpenSim, an open source implementation \& extension of the Second Life server which is compatible with the numerous forks of the Second Life client program. This architecture allows straightforward construction \& dissemination of the models, thanks to accessible modelling tools provided by the Second Life client itself \& the client/server model that allows the models to be accessed in various deployment scenarios including temporary deployments within controlled network \& client conditions as well as remotely via the Internet.

The reconstruction process involves the use of Geographic Information System (GIS) data from Ordnance Survey (OS) to accurately model the basic elevation of the ground. Where there is higher resolution elevation data, such as from Lidar laser surveying often employed on archaeological surveys, this is used to increase the accuracy of the resultant reconstruction. Where access to the site is possible \& depending upon development surrounding the site prior to the date being reconstructed, 360\textdegree\ panoramic photographs are captured \& then used to create a backdrop for the reconstruction, allowing identifiable aspects of the surrounding environment to improve the experience of the reconstruction. Buildings/structures are then reconstructed upon the ground layer, using numerous sources as input; satellite views, archaeological surveys, contemporary accounts, views of the site itself if relics still exist, photographic evidence, etc. Domain experts are then brought in to iteratively improve the model, commenting on aspects of the reconstruction to be altered in order to visualise a different reconstructive hypothesis.

These reconstructions have been used to host workshops at 10 schools throughout Scotland, at both primary \& secondary institutions, where all requisite computing infrastructure was taken, assembled at the school, then disassembled \& removed at the end of the day. Students are split into groups of 4-5, sharing a computer with screen, keyboard, mouse \& Xbox controller (a control modality instantly recognised by most school students). Worksheets with tasks are used to structure their interaction with the reconstructions \& guide the experiential learning experience over 20-40 minute sessions (figure \ref{linlithgow_children.jpg}). Similar workshops have also been performed in museums, using the same approach of temporary setups of computing hardware (figure \ref{musa.jpg}). In addition to traditional computer screens, larger LCD television screens \& still larger projection screens, Oculus Rift VR headsets have been used with this same content~\ref{rift_exhibition.jpg}.

In addition to these temporary workshops, the reconstructions have also been used in permanently installed exhibits in museums \& visitor centres, including the Virtual Time Travel Project (VTTP), which combines multi-head projection with Natural User Interaction (NUI) via Microsoft Kinect, which has been installed at the Timespan Museum \& Arts Centre in Helmsdale, allowing visitors to explore the reconstruction of the Caen Township by using simple gestures, instead of relying upon a keyboard, game controller or other traditional interface (figure \ref{VTTP_projection.png}).

\TwoFig{linlithgow_children.jpg} {School students learning via a reconstruction.} {linlithgow_children.jpg}
       {VTTP_projection.png} {VTTP installation at Timespan.} {VTTP_projection.png}

\TwoFig{rift_exhibition.jpg} {OVW via Oculus Rift.} {rift_exhibition.jpg}
       {musa.jpg} {Museum workshop.} {musa.jpg}

%=========================================================================================================

\subsection{Parallel Reality in Virtual Heritage}
Applications of alternate reality techniques within virtual heritage have thus far broadly fallen into the categories of AR, experienced at the site, or VR, experienced away from the site (in terms of space, time, or both). The dissemination of the OVW group's content has been no exception to this observation, falling into categories 10-12 of figure \ref{taxonomy_2.png}, with complete virtual environments that are experienced with both spatial \& temporal separation from the real site that they represent.

Applying the concept of parallel reality to virtual heritage represents an opportunity to explore an exciting new modality of interaction that combines the complete virtual environments of categories 10-12 with the real time juxtaposition between real \& virtual environments of AR systems from category 13. In terms of the four categories of the taxonomic space, such a PR system would combine the high precision \& interactivity of a VR system (category 10) with two values of viruality, as the user is provided the ability to observe either the complete virtual environment (virtuality = 1) or the unmodified real environment (virtuality = 0). The automatism of such a system would occupy a position between that of VR \& AR; whilst the system will require a more involved development cycle than a purely VR one, the slackened requirements on registrational accuracy of a PR system compared to an AR system promise higher automatism than a purely AR system.

%=========================================================================================================

\section{The Virtual Time Window}
The Virtual Time Window (VTW) is an application of parallel reality to virtual heritage, leveraging the OVW group's existing OpenSim virtual reconstructions of cultural heritage sites in a handheld package that allows tandem exploration of both the real cultural heritage sites \& their spatially equivalent virtual reconstructions.

VTW manifests as a tablet computer which is capable of tracking its position via GPS, its compass heading via magnetometer (`electronic compass') \& its pitch via accelerometer. Existing AR projects have proven through application the suitability of smartphones \& tablets for mobile, position \& orientation aware applications that present virtual content within a cultural heritage context. These devices are also entering ubiquity today \& thus present a platform that can be quickly assimilated by most users. The tablet runs a modified version of the Second Life client in order to access, via wifi, a virtual reconstruction of a cultural heritage site hosted by an OpenSim server. The Second Life client is controlled entirely by the tablet's position \& orientation - the user does not manually control any aspect. The modality of interaction offered is similar to that of using a smartphone to take a photograph, but whereas the screen of the smartphone shows the real environment as it is, the screen of the VTW tablet shows the environment as it was in the past - a window to the past, or \textit{Virtual Time Window}. The user is free to explore the real cultural heritage site, observing it in its current state, whilst at any moment `looking through' VTW to see what a particular vantage looked like in the past.

In terms of the combined Milgram/Waterworth model, the displacement along the locus axis when the user switches their attention between their real environment \& the virtual environment upon the tablet will displace less toward the VR extremum than shown in figure \ref{focus-locus-sensus-with-virtuality-continuum-with-transition} which represents transitions between real \& virtual environments when using a HMD that effectively blocks all stimuli from the real world when observing the virtual. When considering the environmental provision, VTW features two complete environments, one real \& the other virtual. From the perspective of transitioning between receiving stimuli from each environment, there is an obvious difference between VTW's tablet based approach compared to a HMD approach, as the latter effectively forces all percepts to emanate from one environment whilst the former allows percepts emanating from both environments to be perceived simultaneously.

Whilst this will intuitively make transitions easier to perform \& create less risk of a jarring Gestalt switch, it will also intuitively limit the intensity of the sense of presence attainable in the virtual environment as the sense of `looking in to' the virtual environment will always leave the user readily aware of the real environment surrounding them. Whilst VTW is a PR system, one might liken the experience of interacting with it to be similar to that of an AR system.

%Map showing spatial separation of Madras \& cathedral?

%How with VTW we have a small `window' into the virtual, which is then surrounded by the real. So unlike Mirrorshades which is about distributing attention by time (one environment, then the other), VTW is about distributing attention by gaze/place (different parts of a single view/combined environment).

%=========================================================================================================

\subsection{Second Life \& Mobility}

\label{SecondLifeMobility}

\newcommand{\LumiyaFootnote}{\footnote{\url{https://play.google.com/store/apps/details?id=com.lumiyaviewer.lumiya&hl=en_GB}}}

\newcommand{\WindpadFootnote}{\footnote{\url{http://www.msi.com/product/windpad/WindPad-110W.html}}}

%=====================

At the time of the VTW project (Summer 2012) the only fully-featured Second Life clients available were for x86 platforms. Whilst the Android client Lumiya\LumiyaFootnote{} was available, it was in its earliest stages \& very limited in its features \& usability. This limited the choice of tablet to those few x86 models that had reached market, with the MSI WindPad 110W\WindpadFootnote{} presenting the most promising solution: a 10'' tablet sporting an AMD Brazos Z01 APU (combining a dual-core x86 CPU with a Radeon HD6250 GPU).

The Second Life client, intended for use on a desktop or laptop computer, provides provision for controlling the avatar's position \& the camera orientation by keyboard, mouse \& joystick. For the purposes of VTW, this position \& orientation control must be tied to the physical position \& orientation of the tablet itself. To this end, it is necessary to make use of various sensors connected to the tablet (either internally, or externally) \& to interface these with the Second Life client in such a way that it can make use of their collected data to appropriately control the avatar's position \& camera orientation.

%\cite{willmott:largecomplex} occlusion culling etc., missing in Second Life/OpenSim

%Talk about history of Second Life/OpenSim in academia/research, including original cross reality research.

%The 3D virtual environment component of the Pangolin system was implemented using the Second Life/OpenSimulator (SL/OpenSim) platform, which provides a 3D social-oriented multi-user non-competitive virtual environment which focuses on the community, creation and commerce~\cite{Sevan2008} aspects of many users interacting within a shared space through the abstraction of avatars, rather than the competitive natures of games and the solitary environments commonly afforded by simulation and visualization platforms.

%The distributed client/server model of SL/OpenSim, wherein 3D content is stored on a grid of servers operated by a multitude of organizations and distributed to and navigated between by dispersed clients on demand when they enter a particular region rather than being pre-distributed as is the norm for games, simulations and visualizations, is analogous to the manner in which 2D social Web content is served from Web servers to client browsers and apps.

%This style of content delivery is necessary when considering the dynamic and ephemeral nature of consumer-generated media which constitutes the majority of the current 2D social Web and will make up the majority of expanding 3D social Web content.

%=========================================================================================================

\section{Orientation Control}

\newcommand{\ArduinoFootnote}{\footnote{\url{http://www.arduino.cc/}}}

\newcommand{\MMAfootnote}{\footnote{\url{http://cache.freescale.com/files/sensors/doc/data_sheet/MMA8452Q.pdf}}}

\newcommand{\ADXLfootnote}{\footnote{\url{http://www.analog.com/static/imported-files/data_sheets/ADXL335.pdf}}}

\newcommand{\HMCfootnote}{\footnote{\url{http://www51.honeywell.com/aero/common/documents/myaerospacecatalog-documents/Defense_Brochures-documents/HMC5883L_3-Axis_Digital_Compass_IC.pdf}}}

\newcommand{\HMCtwoFootnote}{\footnote{\url{http://www51.honeywell.com/aero/common/documents/myaerospacecatalog-documents/Missiles-Munitions/HMC6343.pdf}}}

%=====================

In order to control Second Life's camera in the fashion required of VTW, sensor data are required for the orientation in which the user is holding the tablet. Specifically, the tablet's yaw \& pitch are needed; roll is less important as it is conceived that the user will generally hold the tablet roughly level with the horizon when looking `through' it.

VTW considers yaw in terms of magnetic compass bearing, as this provides a value that can be used to directly control the yaw of the virtual camera while the virtual reconstruction within OpenSim is correctly oriented to OpenSim's own compass. Magnetic compass bearings are sensed electronically via a 3-axis microelectromechanical (MEMS) magnetometer, which measures the strength of magnetic field being experienced along each of its 3 axes. By comparing the values of each axis to the known direction of the field lines of Earth's magnetic field, a compass bearing relative to the magnetometer's orientation can be calculated. Pitch is sensed using a 3-axis MEMS accelerometer, which measures force of acceleration along each of its 3 axes. In the case of static or slow moving applications, this acceleration is predominantly that caused by the Earth's gravitational pull \& by comparing the values of each axis the direction of this acceleration (down toward the centre of the Earth) can be determined in relation to the orientation of the accelerometer itself \& thus the accelerometer's own orientation can be deduced.

Due to the fact that the WindPad tablet does not feature a built-in magnetometer \& its built-in accelerometer is little more than a rudimentary tilt sensor for differentiating between discrete cases of landscape and portrait orientation for screen rotation, it was necessary to interface external magnetometer \& accelerometer sensors. The popular Arduino\ArduinoFootnote{} microcontroller platform was used for prototyping with several different sensor packages, including the MMA8452\MMAfootnote{}, the ADXL335\ADXLfootnote{} \& the HMC5883L\HMCfootnote{}. The package adopted for use with VTW from this prototyping stage was the HMC6343\HMCtwoFootnote{}, which combines a 3-axis MEMS magnetometer \& 3-axis MEMs accelerometer into a single package, along with algorithms to internally apply the accelerometer's readings to `tilt compensate' the magnetometer's readings. Figure \ref{arduino_wiring_hmc.png} provides a wiring diagram for connectivity of a HMC6343 to an Arduino Uno R3, with the pinout values provided by figure \ref{HMC6343wiringtable} \& Figure \ref{arduino_joystick_for_second_life_1.jpg} shows an assembled unit.

\begin{figure}[h]
\centering
  \includegraphics[width=\linewidth]{arduino_wiring_hmc.png}
  \caption{Example wiring for Arduino with HMC6343 for joystick operation.}
  \label{arduino_wiring_hmc.png}
\end{figure}

%=====================

\begin{figure}[h]
\begin{center}
\begin{tabular}{| c | c |}

\hline	

\textbf{HMC6343 pin} & \textbf{Arduino Uno R3 pin} \\

\hline

VCC/VDD & 5V \\

\hline

GND & GND \\

\hline

SDA & A4 \\

\hline

SCL & A5 \\

\hline

\end{tabular}
\end{center}
\caption{Pin designation for figure \ref{arduino_wiring_hmc.png}}
\label{HMC6343wiringtable}
\end{figure}

%=====================

\begin{figure}[h]
\centering
  \includegraphics[width=.7\linewidth]{arduino_joystick_for_second_life_1.jpg}
  \caption{Assembled Arduino Uno R3 + HMC6343.}
  \label{arduino_joystick_for_second_life_1.jpg}
\end{figure}

A magnetometer used alone is only capable of providing a meaningful compass bearing when held level. In the case of applications where a compass bearing is required of a device that is not maintained level, such as in the case of VTW, the non-level orientation of the device must be taken into account to offset the readings of the magnetometer \& provide a correct compass bearing. The HMC6343's combination of magnetometer, accelerometer \& algorithms provides a single package that internally performs this process, using the readings from its accelerometer to compensate the readings from its magnetometer \& provide a meaningful compass bearing in non-level orientations.

Further requirements for obtaining accurate compass bearings from a MEMS magnetometer are to account for distortions to the magnetic field it senses \& to compensate the bearings it reports for the amount of magnetic declination for the location \& date wherein it is being used. Various materials that influence magnetic fields or produce their own magnetic field will distort the Earth's magnetic field \& thus impact the readings that a MEMS magnetometer collects. In the case of VTW, the sources of primary consideration are the electronics of the Arduino, tablet \& associated wiring. Due to the nature of these sources \& the fact that they are permanently situated \& attached to the same frame of reference as the magnetometer, moving as it moves, the distortions can be mitigated using a hard iron offset approach. Magnetic declination refers to the difference between `magnetic north' \& geographic `true north'. This value varies depending upon world location \& changes over time, so must be updated when the magnetometer is deployed to a different location or used at a subsequent date.

%***Reference for hard iron offset - here or later where we talk about the hex?

%=========================================================================================================

\subsection{Exploiting Second Life's Joystick Support}

\newcommand{\ArduinoJoystickVideoFootnote}{\footnote{\url{https://www.youtube.com/watch?v=-ddtmqoGNmg}}}

\newcommand{\atmegaFootnote}{\footnote{\url{http://www.atmel.com/devices/ATMEGA16U2.aspx}}}

\newcommand{\atmegaTFootnote}{\footnote{\url{http://www.atmel.com/devices/atmega328.aspx}}}

\newcommand{\arduinousbhidFootnote}{\footnote{\url{http://hunt.net.nz/users/darran/weblog/a3599/}}}

\newcommand{\lufaFootnote}{\footnote{\url{http://www.fourwalledcubicle.com/LUFA.php}}}


%=====================

As highlighted in section \ref{SecondLifeMobility} the Second Life client can be controlled only via mouse, keyboard \& joystick. Using the HMC6343's compass bearing \& yaw values therefore requires one of two approaches;

\begin{enumerate}
	\item Encapsulating the compass bearing \& yaw values into mouse, keyboard \&/or joystick commands;
	\item Modification to the Second Life client to allow the compass bearing \& yaw values to be used directly at a lower level of abstraction.
\end{enumerate}

Method 1 has the advantage of having no reliance upon any particular Second Life client, as all available clients are forks of the official client from Linden Lab \& maintain the same keyboard, mouse \& joystick interfaces. However if the level of control attainable by re-purposing these interfaces for control from magnetometer \& accelerometer data is not enough, method 2 will be the only option.

Conceptually, all Arduino boards are programmed over an RS-232 serial connection. When the platform was first launched, the Arduino boards themselves had a physical DE-9 serial connector with which to connect to a host computer's serial connector. But as serial connectors all but disappeared from modern computers, the Arduino's serial connector was replaced in later revisions with a USB connector, as USB is now all but ubiquitous on today's computers. The move from a physical RS-232 connector to a USB connector requires additional hardware upon the Arduino board to convert between RS-232 \& USB, as the ATMega328\atmegaTFootnote{} microcontroller at the heart of the Arduino Uno R3 does not have a USB interface itself. For this reason the current revision, the Arduino Uno R3, sports an ATMega16U2\atmegaFootnote{} microcontroller that serves to convert communications between the two standards, RS-232 \& USB.

With its stock firmware, the Arduino's ATMega16U2 presents itself to the host computer as a USB-to-serial bridge. However the ATMega16U2 can have this stock firmware replaced in order to change its behaviour. One of these new behaviours is to act as a USB Human Interface Device (HID) class controller, identifying itself to the host computer as one of a myriad input devices - including joysticks. Using a USB HID class joystick firmware for the ATMega16U2\arduinousbhidFootnote{}, based upon the Lightweight USB Framework for AVRs (LUFA)\lufaFootnote{}, the Arduino can imitate a standard USB joystick, sending joystick commands to the host computer using the protocol in the USB specification.

%***How to flash the firmware

In this manner, the Arduino can marshal the values obtained from the HMC6343 into standard USB HID joystick commands, allowing the Second Life client's stock joystick interface (see figure \ref{arduino_joystick_for_second_life_3.jpg}) to be used to control the camera orientation (\& avatar movement) according to the physical orientation of the HMC6343, as can be seen in\ArduinoJoystickVideoFootnote{}.

\begin{figure}[h]
\centering
  \includegraphics[width=\linewidth]{arduino_joystick_for_second_life_3.jpg}
  \caption{Configuration in Second Life client for Arduino + HMC6343 `joystick'.}
  \label{arduino_joystick_for_second_life_3.jpg}
\end{figure}

Unfortunately, the precision attainable through this approach is not sufficient for the style of control \& interaction required for VTW. Specifically, the Second Life client's joystick interface applies smoothing/damping to the joystick inputs, preventing reliable rotations or movements of specific values - sending a joystick command to rotate the camera by $x$ degrees followed by a second command to rotate the camera by $-x$ degrees does not reliably return the camera to its original position. As the interaction required is to map the \textit{absolute} orientation of the tablet to the Second Life camera, this discrepancy (which cannot be disabled from the Second Life client's joystick configuration) renders the approach unworkable.

%=========================================================================================================

\section{Position Control}

\textbf{***Include HEX whatnot from Arduino code}

\textbf{***Include Haversine as pseudocode(?), RegionModule code \& Second Life client modification}

%Magnetic declination information was entered into the HMC6343 for the position of the cathedral and the date of our experiments. The HMC6343's hard-iron offset calculation feature was used each time the hardware configuration was altered. The sampling frequency of the HMC6343 was set to its highest value of 10Hz. Orientation was set to `upright front' to match the physical orientation of the IC in the experiments.

%\begin{figure}[h!tbp]
%\begin{lstlisting}[language=Java, numbers=left, numberstyle=\tiny, stepnumber=5]

The user's position is monitored using GPS, a solution which is well suited to applications of the system within the use case of cultural heritage; such sites often constitute outdoor ruins at which a clear view of the sky allows for good GPS connectivity. For use cases where a similar modality of interaction is desired whilst indoors then an indoor positioning system would be used; a roundup of such technologies is available in~\cite{Mautz2012}.

Translating real world positions, obtained via the GPS receiver as latitude and longitude pairs, into corresponding OpenSim (X,Y) region coordinates is achieved using the haversine formula~\cite{Gellert1989} from spherical trigonometry. The prerequisites for this approach are that the OpenSim model is aligned correctly to the OpenSim compass as the real location is aligned to real bearings (although provision to specify an `offset' within the Pangolin viewer for non-aligned models would be a trivial addition), that the model was created to a known and consistent scale and that a single `anchor point' is known for which both the real world latitude/longitude and corresponding OpenSim (X,Y) region coordinates are known.

Using the haversine formula the great-circle (or orthodromic) distance between the latitude of the anchor point and the latitude of the new GPS reading is calculated, then applying the scale of the model results in the equivalent distance in OpenSim metrics between the Y coordinate of the anchor point and the Y coordinate of the position corresponding to the new GPS reading. Repeating the same calculations with the longitude of the new GPS reading provides the distance between the X coordinate of the anchor point and the X coordinate of the position corresponding to the new GPS reading. Adding or subtracting these distances as appropriate to the OpenSim coordinates of the anchor point provides the OpenSim coordinates that correspond to the new GPS reading, to which the avatar is then instructed to move.

The anchor point is specified using global coordinates, not local coordinates. This allows navigation to operate across region boundaries and within mega regions (it is not limited to a single 256x256 meter OpenSim region) and there are no restrictions for the placement of the OpenSim component of the anchor point (it can be anywhere in any region, movement of the avatar can be in any direction from it (positive and negative), it does not have to be at the center of the model or even in a region that the model occupies).

Calculating a global coordinate is simply a case of multiplying the position of the region by 256 and then adding the local coordinate. For example, for an anchor at local coordinate $(127,203,23)$ within a region that is at $(1020,1042)$ the global X coordinate is calculated as $(1020 * 256) + 127 = 261247$ and the global Y coordinate as $(1043 * 256) + 203 = 267211$. Elevation (Z) is ignored due to a combination of the relatively low accuracy of these data attainable via GPS (when compared to the longitudinal/latitudinal accuracy) and as the case study explored involved users navigating outdoor ruins remaining at ground level.

%=========================================================================================================

\subsection{OpenSim RegionModule}

%=========================================================================================================

\section{Modifying Second Life for Orientation \& Position Control}

%for both orientation \& position

\textbf{***Include code \& structure of modifications from Second Life client itself}

The 110W features an AzureWave GPS-M16~\cite{AzureWave} GPS receiver; however poor API provision and meager documentation lead to use of a separate u-blox MAX-6 GPS receiver~\cite{U-bloxAG} outfitted with a Sarantel SL-1202 passive antenna~\cite{Sarantel}. The MAX-6 is of higher operational specification than the GPS-M16 and supports Satellite Based Augmentation Systems (SBAS) which improve the accuracy of location data by applying additional correction data received from networks of satellites and ground-based transmitters separate to those of the GPS system. These networks include the European Geostationary Navigation Overlay Service (EGNOS) that covers the UK where the experiments took place.

The product summary for the MAX-6 claims accuracy of 2.5m Circular Error Probable (CEP) without SBAS corrections and 2m CEP with SBAS corrections ``demonstrated with a good active antenna''~\cite{U-bloxAG2012}. This means that, in an ideal situation with SBAS correction data available, there would be 50\% certainty that each position reported by the GPS receiver would be within 2m of its actual position. The SL-1202 antenna used is passive, however as the distance between antenna and the MAX-6 IC itself in the hardware application is only a few millimeters there would have been negligible benefit from using an active antenna. However whether the SL-1202 constitutes `good' for achieving the headlining performance characteristics of the MAX-6 is debatable as the definition of `good' was not provided in the product summary.

The MAX-6 was operated in `pedestrian' dynamic platform model, use of SBAS correction data was enabled and frequency of readings was set to the maximum of 5Hz.

To determine the real world accuracy attainable with the MAX-6 outfitted with the SL-1202 in situations akin to those of the cultural heritage case study, a walking route around the St Andrews cathedral ruins, akin to the route that an individual visitor or school group might take, was planned and then walked with the MAX-6 connected to a laptop computer via an Arduino operating as a Universal Asynchronous Receiver/Transmitter (UART) feeding the raw National Marine Electronics Association (NMEA) messages into the `u-center'GPS evaluation software version 7.0 which logged the messages for later evaluation. Simultaneously for comparative purposes a mid-range consumer Android smartphone was used to record the same track; a HTC One S~\cite{HTCCorporation2013} containing a gpsOne Gen 8A solution within its Qualcomm Snapdragon S4 processor~\cite{QualcommIncorporated2013} and using Google's `My Tracks' app version 2.0.3 to record the data. The three sets of positional data (planned route, MAX-6 recorded route and smartphone recorded route) were entered into a PostgreSQL database~\cite{Daviesc,Daviesb} and the PostGIS database extender's ST\_HausdorffDistance algorithm~\cite{PostGIS} was used to calculate the Hausdorff distances between the recorded routes and the planned route and between the recorded routes themselves. In this scenario, the Hausdorff distance represents the furthest distance needed to travel from any point on the route recorded by the GPS receiver to reach the nearest point on the planned route. Because of the substantially greater inaccuracies identified in the latter part of the recorded tracks, separate Hausdorff distances were calculated both for the complete tracks and also for truncated first and second sub-tracks.

Whilst SL/OpenSim encapsulates many of the desirable architectural features for 3D PoSR experiments it does not support execution upon familiar mobile platforms (Android/iOS) nor does it provision for avatar control from sensor data. However the open source nature of the SL viewer allowed modifications to be effected, enabling control of the avatar and camera from real time data collected from position and orientation sensors connected to a tablet computer. This ability to control navigation within the 3D virtual environment without explicit conscious input of keyboard/mouse/touch commands is integral to reducing the cognitive load required to maintain a presence within a virtual environment which is a key requirement for overcoming the vacancy problem and achieving successful mobile cross reality.



To reduce computational load on the 110W, the OpenSim server was run on a separate Lenovo ThinkPad X61s laptop computer during the experiments. Due to the limited range of the laptop's wireless interface, the laptop was connected by RJ45 ethernet cable to a Linksys WRT54G wireless router to allow the 110W to access the OpenSim server wirelessly from anywhere within the experiment area. The router was powered from a 12V sealed lead-acid battery. This setup is shown in figure \ref{server}.



The TinyGPS library~\cite{Hart} was used to abstract processing of NMEA messages from the MAX-6 to obtain the required latitude and longitude values.

HMC6343 SDA A4
HMC6343 SCL A5

ublox TX digital 5
ublox RX digital 4

LED on digital 12

5v \& gnd where appropriate (boards used have step downs to 3.3v where required)

Go through Arduino sketch \& talk about things like magnetic declination, etc.

\begin{figure}[h]
\centering
  \includegraphics[width=\linewidth]{arduino_wiring_hmc_ublox.png}
  \caption{Example wiring for Arduino with HMC6343 + u-blox MAX-6.}
  \label{arduino_wiring_hmc_ublox.png}
\end{figure}

\TwoFig{arduino_hmc6343_u-blox_MAX-6.jpg} {Assembled Arduino/sensors.} {arduino_hmc6343_u-blox_MAX-6.jpg}
       {pangolin_tablet_back.jpg} {Arduino/sensors attached to tabet.} {pangolin_tablet_back.jpg}

the viewer was modified (giving rise to the Pangolin viewer) to make use of the Boost.Asio C++ library to support receiving data via serial port and to use these data to control the movement of the avatar and camera by directly interfacing with the control functions at a lower level of abstraction. Receipt of messages is performed in an asynchronous non-blocking fashion, with the viewer's main loop processing the most recently received message in each iteration. Messages follow the format

$\langle bearing \rangle$ $\langle pitch \rangle$ $\langle roll \rangle$ $\langle latitude \rangle$ $\langle longitude \rangle$

The viewer's GUI was modified with the addition of a dialogue that allows the user to specify the path of the serial device, separately enable or disable sensor-driven camera and movement control, as well as providing numerous controls for fine-tuning its behavior, including the ability to specify high-pass filters for avatar movement and specify the smoothing applied to camera control. This GUI also presents the necessary fields for input of the anchor point details and fields for diagnostic output of the received information. Figure \ref{pangolin_screenshot} shows this GUI within the Pangolin viewer.

\begin{figure}[h]
\centering
  \includegraphics[width=0.7\linewidth]{pangolin_second_life_dialogue.png}
  \caption{Config pane in modified Second Life client for HMC6343 + MAX-6.}
  \label{pangolin_second_life_dialogue.png}
\end{figure}

%=========================================================================================================

\section{Using the Virtual Time Window}

\TwoFig{pangolin_laptop_router_battery.jpg} {OpenSim Server \& wireless AP.} {pangolin_laptop_router_battery.jpg}
       {pangolin_in_use.jpg} {VTW at the cathedral.} {pangolin_in_use.jpg}

Two plausible modalities of interaction were identified for this system, with each presenting different requirements with regards to accuracy of position tracking.

The first modality is one in which a number of locations that represent points of particular interest are identified. This is already a common practice at cultural heritage sites, with such locations often bearing signs or placards presenting text and/or images explaining what can be observed from the position. With Pangolin, when a user walks within a certain range of such a point, their avatar can be moved to the corresponding location within the reconstruction (and a sound played to alert the user to the fact that there is something of interest to observe) from which they can then move the tablet around them to examine their surroundings in the reconstruction. This modality is similar to audio tours employed by many museums and cultural heritage sites, but replaces the requirement to follow a static route or type in numbers of locations with the ability to freely navigate the real environment with access to additional information being triggered automatically once within the required range of a point of interest.

The second modality is one of free roaming exploration, in which the movements of the user's avatar within the reconstruction mimic the user's movements within the real world as closely as possible.
The first modality can be scaled to function with different accuracies of position tracking; as long as the distance between any two points of interest is at least as much as the worst case performance of the position tracking then distinguishing correctly between different points will always succeed. The second modality requires extremely accurate position tracking, arguably surpassing the capabilities of mainstream GPS technology even in ideal situations.

During the experiments the MAX-6 was unable to maintain reception of the additional correction data required for SBAS operation; when left stationary for several minutes reception was possible however subsequent movement of only a few meters at walking pace broke the connection. This reduced the theoretical maximum performance of the unit to 2.5m CEP, with observed performance being lower. Figure \ref{map_one} depicts an aerial view of the St Andrews cathedral ruins; the blue line represents the planned route, red the route recorded by the MAX-6 receiver and green the route recorded by the smartphone for comparative purposes, both while walking the planned route.

%\begin{figure}[h]
%\centering
%\includegraphics[width=0.48\textwidth]{images/figure_4}
%\caption{Aerial view oriented North upward of the St Andrews cathedral ruins; the blue line represents the planned route, red the route recorded by the MAX-6 and green the route recorded by the smartphone whilst walking the planned route.}
%\label{map_one}
%\end{figure}

The Hausdorff distance between the planned route and that recorded by the MAX-6 was $1.02e^{-04\circ}$. The `length' of a degree of latitude and a degree of longitude depends upon location upon the Earth; around the location of the St Andrews cathedral 1$^\circ$ of latitude is equivalent to 111347.95m and 1$^\circ$ of longitude to 61843.88m. Thus the Hausdorff distance of $1.02e^{-04\circ}$ can be visualized as $\pm11.3$m of North/South inaccuracy or $\pm6.3$m of East/West inaccuracy (or a combination of both N/S and E/W inaccuracy not exceeding a total displacement of $1.02e^{-04\circ}$ from the planned route).

The MAX-6 did achieve better performance than the smartphone, which recorded a Hausdorff distance of $1.33e^{-04\circ}$ ($\pm14.8$m N/S, $\pm8.2$m E/W). The Hausdorff distance between the routes logged by the MAX-6 and the smartphone was $1.14e^{-04\circ}$ ($\pm12.7$m N/S, $\pm7.0$m E/W), which represents a low correlation between the inaccuracies recorded by the two receivers even though they are of similar magnitudes from the planned route.

The maximum inaccuracies were recorded when walking along the South wall of the cathedral's nave. This wall is one of the most complete sections of the building with stonework reaching some 30ft above ground level and providing an effective obstruction to line-of-sight to half of the sky (and substantially impairing reception of signals from GPS satellites) when in close proximity to it. When considering just the sub-route shown in figure \ref{map_two}, which terminates before this wall begins to significantly obstruct view of the sky, the Hausdorff distances are notably smaller; the MAX-6 achieved a Hausdorff distance of $7.23e^{-05\circ}$ ($\pm8.05$m N/S, $\pm4.47$m E/W) throughout this sub-route, with the smartphone still behind with $8.99e^{-05\circ}$ ($\pm10.01$m N/S, $\pm5.56$m E/W). Again the Hausforff distance between the receivers showed low correlation between the inaccuracies, at $6.43e^{-05\circ}$ ($\pm7.12$m N/S, $\pm3.98$m E/W).
 
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.48\textwidth]{images/figure_5}
%\caption{Aerial view oriented North upward of the St Andrews cathedral ruins; the blue line represents the first sub-route of the planned route, red the sub-route recorded by the MAX-6 and green the sub-route recorded by the smartphone whilst walking the first planned sub-route.}
%\label{map_two}
%\end{figure}

When analyzing the tracks in the vicinity of the nave (see figure \ref{map_three}) it is shown that although the MAX-6 outperformed the smartphone in terms of Hausdorff distance this relationship can be considered misleading as the smartphone track corresponded more closely in shape to the planned route even if it did stray further at its extreme. The discrepancy in the behavior of the two receivers in this situation is attributed to different implementations of dead-reckoning functionality between the receivers. Dead-reckoning is the process used when a GPS receiver loses reception of location data from satellites and extrapolates its position based upon a combination of the last received position data and the velocity of travel at the time of receiving these data.
 
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.48\textwidth]{images/figure_6}
%\caption{Aerial view oriented North upward of the St Andrews cathedral ruins; the blue line represents the second sub-route of the planned route, red the sub-route recorded by the MAX-6 and green the sub-route recorded by the smartphone whilst walking the second planned sub-route.}
%\label{map_three}
%\end{figure}

Pangolin's camera control from orientation data does not have as stringent performance criteria as the movement control from position data. Unlike augmented reality where sparse virtual content is superimposed upon a view of a real environment and the virtual objects must be placed accurately in order for the effect to work well, cross reality presents a complete virtual environment that is viewed `separately' or side-by-side with the real environment and thus discrepancies between orientation of real and virtual environments have a less detrimental effect to the experience. Although the accuracy of the camera control during the experiments was reported as being sufficient, the speed at which the camera orientation moved to match physical orientation was reported as being too slow, resulting in having to wait for the display to `catch up' to changes in orientation. This is attributed to the 10Hz sampling rate of the orientation sensors which, particularly after readings are combined for smoothing purposes to reduce jerky movement, resulted in too infrequent orientation updates. Frame rates within Pangolin whilst navigating the route averaged between 15 and 20 frames per second with the viewer's `quality and speed' slider set to the `low' position.

The style of explorative interaction with virtual content that this system employs is more resilient to input lag and low frame rates than other scenarios of interaction with virtual content such as fast paced competitive video games including First Person Shooters (FPS) [20], but overall user experience would nonetheless be improved by a faster sampling of orientation data and a higher frame rate. Additionally it should be noted that the cathedral reconstruction was created with relatively powerful desktop computers in mind as the primary deployment platform and has not been optimized for use on less powerful mobile platforms such as Pangolin. Performance of Pangolin on a less graphically complex OpenSim region (Salt Pan 2 [17]), that also depicts a reconstruction of a cultural heritage site, was better at 20 to 25 frames per second at the `low' position and between 15 and 20 frames per second at `high' (see figure 7).

%\begin{figure}[h]
%\centering
%\includegraphics[width=0.48\textwidth]{images/figure_7}
%\caption{Plot of Pangolin's performance (measured in frames per second) against different graphical settings (selected via the `Quality and speed' slider of the viewer) in two positions within the Salt Pan 2 region.}
%\label{framerate_graph}
%\end{figure}

%\section{Interpretations}
The positional accuracy of $1.02e^{-04\circ}$ attained by the MAX-6 is sufficient for the first modality of interaction (that of distinguishing and navigating between multiple points of interest). This value of $1.02e^{-04\circ}$ (analogous to a combination of $\pm11.3$m of North/South inaccuracy or $\pm6.3$m of East/West inaccuracy) represents a constraint on the granularity of the content; it is the minimum distance required between any two points of interest for them to be correctly differentiated between. This same value is not sufficient for the second modality of interaction (that of free roaming exploration with avatars mimicking their users' movements as closely as possible). This modality would require the use of additional position tracking techniques to improve accuracy to around 1m CEP (analogous to $8.98e^{-06\circ}$ latitude or $1.62e^{-05\circ}$ longitude around the location of the St Andrews cathedral).

Use of a GPS receiver that is lower performance than the MAX-6 used by Pangolin, but more common due to being of the calibre integrated into smartphones and tablets such as that used in the experiments, is still sufficient for the first modality but with a larger minimum distance required between any two points of interest. The Hausdorff distance of $1.33e^{-04\circ}$ recorded by the smartphone used in the experiments is analogous to $\pm14.8$m N/S or $\pm8.2$m E/W around the location of the cathedral.

Observed accuracy of the orientation tracking is sufficient for both modalities of interaction; the accuracy of orientation tracking required does not change with different positional accuracy and the accuracy of orientation attained in the experiments is sufficient for an acceptable user experience, however the experience would benefit from better graphical quality and higher responsiveness to changes in user orientation.

%=========================================================================================================