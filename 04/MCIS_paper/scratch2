

\subsection{Microsoft SenseWeb}
SenseWeb from Microsoft Research is a system that allows peer production of sensing applications, producing new kinds of media and applications over existing data networks, by allowing users to grant access to their sensors to other remote users. The basic premise is that contributors deploy their own sensors or sensor networks, which might be designed for their own dedicated application or simply for sharing with other users, uploading the observations from these sensors to the SenseWeb system where they can then be accessed by other SenseWeb users through an application-specific GUI. Using SenseWeb, applications can initiate and access sensor data streams from shared sensors across the entire Internet~\cite{kansal:senseweb}.

The approach of the system is such that applications can access resources contributed by any entity in a uniform manner, regardless of the underlying platform of that entity. To achieve this, sensors are connected to a sensor gateway which provides a uniform interface to all components above it, hiding much of the complexity of the heterogeneous communication protocols, power capabilities, bandwidth capabilities, etc. of the numerous sensor platforms beneath.

However unlike with systems that implement IEEE 1451 compliant smart transducers, when a developer wishes to connect sensors to a sensor gateway using SenseWeb she must manually define many of the characteristics and properties of these sensors; type, location (longitude, latitude, altitude), description, etc. Additionally for the observations from different sensors to be made useful (eg two different types of temperature sensor, one that reports observations simply as a voltage from a thermistor and the second that performs some on-sensor processing and calibration and reports observations as a value in centigrade) a `data transformer' must be used to convert data semantics through processing. Whilst this approach to using intermediary data transformation allows for powerful handling of data, such as passing a video observation through a people count algorithm and only presenting to other users the output of this algorithm rather than the video itself, it adds complexity and manual implementation to accessing and making sense of observations which could have been achieved via a system such as the TEDS of IEEE 1451. When a user wishes to connect sensors to a sensor gateway, there is no automatic discovery by the sensor gateway of these sensors, nor of their characteristics (available sensors/actuators, calibration, type of communications interfaces, etc.)~\cite{microsoft:sensewebtutorial}.

Communication between sensors and sensor gateways relies upon the existence of drivers. Whilst many common sensing devies, including wireless motes and network cameras, already exist, this approach means that scalability to new and particularly unusual or unpopular (in terms of widespread adoption) sensing platforms is poor. Whilst SenseWeb provides some very powerful techniques at the application interface level for abstracting over heterogeneity at lower levels, such as differences in requirements to conserve power and bandwidth, by making extensive use of caching and distribution of tasking to different individual sensors when many are available that can achieve the same observation, all of this abstraction assumes that the sensors themselves have already been integrated with the system, usually requiring manual human intervention.

SenseWeb presents a very polished interface to the end user, making it extremely easy and intuitive to pull in observations from sensory resources around the globe and put them to good use, however this ease of interaction at the end user level comes at a price lower down where sensor attributes must sometimes be manually specified and intermediary processing must sometimes be implemented. SenseWeb serves as a good example of what the interface to a standardised framework for cross reality development should be like, however automatic resource discovery and administration at the lower levels would make the system more desirable to developers at all levels, rather than just at the receiving end of observations.

\subsection{Global Sensor Network (GSN)}
GSN is a middleware developed by \'{E}cole Polytechnique F\'{e}d\'{e}rale de Lausanne (EPFL) to support the rapid and simple deployment of a wide range of sensor network topologies, facilitate the flexible integration and discovery of sensor networks and sensor data, enable fast deployment and addition of new platforms, provide distributed querying, filtering and combination of sensor data and support dynamic adaptation of the system configuration during operation~\cite{aberer:gsn}.

EPFL researchers realised that as the price of wireless sensors decreased and the number of autonomous sensor networks increased, the emerging desire for a `Sensor Internet' was being hampered by the difficulty of sharing and integrating data among heterogeneous sensor networks, as previous research in the sensor network domain had mainly focused on routing, data aggregation and energy conservation inside a single sensor network.

After identifying the standard strategy adopted successfully in other domains of addressing such a problem by devising a middleware which provides powerful abstractions allowing easy integration of concrete physical platforms, EPFL set about developing GSN.

GSN's key abstraction is the `virtual sensor', which abstract from implementation details of access to sensor data and corresponds either to a data stream received directly from a sensor or to a data stream derived from other virtual sensors. Virtual sensors can be any kind of data producer; actual sensors, wireless cameras, desktop computers, mobile phones, or even a combination of these. Each virtual sensor can have any number of input streams but produces exactly one output stream based on these input streams and some logical processing.

Virtual sensors are described in a declarative deployment descriptor, which is an XML file defining various attributes; a unique identifier, any number of input streams, the structure of the output stream, etc. Defining this XML file is all that is required to deploy a new sensor as long as GSN includes software support for the concerned hardware/software. This support is provided by wrappers conforming to the GSN API and several such wrappers are provided to cover common hardware/software including a generic HTTP wrapper for accessing data from devices using HTTP GET and POST, and a serial forwarder wrapper that supports interaction with TinyOS WSN motes over a direct serial connection. Implementing new wrappers is not a substantial endeavour either, as even complex wrappers are only a few hundred lines of code (mostly Java).

GSN has been demonstrated on real hardware through experiments with typical high-load application profiles and is used as the primary streaming data database for the ETH Centre for Competence Environment and Sustainability (CCES) and through the Swiss Experiment platform project the usage of GSN is increasing, currently in the region of 80 $-$ 100 million data points.

GSN is also open source under the GNU GPL available freely on the Web~\cite{gsn:url}, making it ideal to take and extend for cross reality middleware.

\textcolor{red}{summary of how it could/can/has (be(en)) used for cross reality}

\subsection{MPEG-V}
The currently under development MPEG$-$V standard, `Information Exchange with Virtual Worlds', aims to address this shortcoming, by defining standards for information and media exchange between virtual worlds and interfaces between virtual worlds and the real world.

\begin{quotation}
	\textit{``The `Information exchange with Virtual Worlds' project intends to provide a standardized global framework and associated interfaces, intermediate formats definitions and the like, to enable the interoperability between virtual worlds (as for example Active Worlds, Second Life, IMVU, Google Earth, Virtual Earth and many others) and between virtual worlds and the real world (sensors, actuators, vision and rendering, robotics (e.g. for revalidation), (support for) independent living, social and welfare systems, banking, insurance, travel, real estate, rights management and many others).''} \\ \textcolor{red}{cite (Summary of MPEG-V, International Organization for Standardization)}
\end{quotation}

The standard is subdivided into 6 parts, of which particular interest for cross reality developers are part 2, Control Information, and part 3, Sensory Information. Part 2 defines interaction using XML for controlling a myriad of physical and environmental properties from simple concepts such as `Light capability type' to more advanced concepts such as `Fog capability type'. Likewise part 3 defines XML-based interaction with sensing primitives from `Temperature effect' to `Passive Kinesthetic Motion effect'.
