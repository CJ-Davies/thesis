Past augmented reality and virtual reality projects within the realms of archaeology and cultural heritage have made use of a myriad of software and hardware packages and interaction standards, including but not limited to the virtual worlds Second Life and OpenSimulator~\cite{ruffaldi:informationlandscapes}, web-based technologies including Adobe Flash~\cite{laycock:time}, VRML~\cite{vlahakis:archeoguide} and its successor X3D~\cite{cabral:x3dexperience}, `traditional' computer graphics APIs such as OpenGL~\cite{laycock:time, willmott:largecomplex, roussou:photorealism} and rendering packages such as Maya~\cite{sundstedt:highfidelity}, game engines such as Unity3D\cite{deamicis:gamebased}, packages targeted specifically to augmented reality and virtual reality applications such as XVR~\cite{ruffaldi:informationlandscapes} and CAVE technologies including ReaCTor~\cite{christou:largescale} and even highly specialised packages such as Radiance~\cite{sundstedt:highfidelity}, a suite of software tools for performing extremely accurate lighting simulation.













\st{The suitability of these technologies to building cross reality systems in general, but with a focus on those for the exploration of cultural heritage sites, are surveyed and the technologies are presented by means of a taxonomy that classifies them according to which of the functional components of such systems they address, their progress toward deployment and their level of generality (that is, whether they specifically target cross reality development, or are more general tools amenable to such usage).}





\textcolor{red}{\section{Technologies(?)}}
\label{sec:technologies}
Blurb about taxonomy, how it is divided up/how the categorization works etc.


\textcolor{red}{The advent of virtual worlds which allow simulation of real world locations did not spawn the demand for a framework that facilitates synchronous bidirectional flow of sensory and control information between sensor/actuator infrastructure and display technologies (be that display a virtual world or a traditional graph). This style of interaction has existed for a long time, as sensor networks were employed by numerous fields long before the adoption of virtual worlds as a research tool.}

\textcolor{red}{This discussion has thus far somewhat falsely presented a sensor network as an entity producing simplex communication, relaying sensor readings from nodes to a central authority in an autonomous fashion. In reality this is seldom a desirable interaction model as some level of control over the sensor network is usually required to affect which sensors are utilized and how they behave.}

\textcolor{red}{As such there are a plenitude of existing projects and standards that already facilitate bidirectional communication of sensory and control information to and from networks of \textit{transducers}; devices that convert energy from one form to another. Transducers may be thought of as a super set of sensors and actuators; a sensor is a transducer that generates an electrical signal proportional to a physical, biological or chemical parameter (such as an increasing voltage in response to an increasing level of ambient light), whilst an actuator is a transducer that accepts an electrical signal and takes some physical action (such as switching an electrical relay in response to a certain electrical signal)~\cite{lee:understanding}.}

\textcolor{red}{These standards do not necessarily envisage that they will be used to effect discernible changes in the physical environment in the manner that cross reality pursues, rather that the communication from computer to transducers will be used to administrate their sensory and communicative behaviour. However from a technical perspective using this communication to trigger a transducer into activating one of its sensors is little if at all different from using it to trigger a transducer into activating a relay that controls a high-powered electrical device such as an air conditioning unit. To labour the analogy further, controlling a camera's pan and tilt motors can be considered functionally identical to controlling motors that open a door.}

\textcolor{red}{Whilst implementations of these existing standards are not suitable as they stand to serve as a complete solution for the model of interaction that cross reality requires, it is important to appreciate their existence and that they may be able to serve as building blocks for an eventual solution, either by directly providing functionality that can be extended and/or by providing architectures, deployment models, etc.}

\textcolor{red}{This section continues by describing several projects and standards that provide this bidirectional communication between computer and transducers, which are not explicitly intended for cross reality usage but can offer developers of cross reality some help. In addition, past projects that have directly addressed the subject from a cross reality perspective are identified.}


\subsection{Functional Components of a Cross Reality System}
identify functional components that go to making a cross reality system $-$ will allow systems libraries to be classified according to which functional components they address

\textcolor{red}{draw a diagram/table}

\begin{itemize}
	\item \textbf{Transducer Infrastructure} $-$ Discussed in section \ref{sec:transducerinfrastructure}
	\item \textbf{Virtual Environment} $-$ Discussed in section \ref{sec:virtualenvironments}
	\item \textbf{Middleware} $-$ Discussed in section \ref{sec:middleware}
\end{itemize}

\subsection{Progress Towards Deployment}
progress towards deployment $-$ is it just an idea, a ratified standard, is there code and implementations, has it been deployed and evaluated

\subsection{Applicability to Cross Reality}
is it designed specifically for cross reality, or is it just something that can be hacked and misused for cross reality (see previous point)

\textcolor{red}{\section{Transducer Infrastructure}}
\label{sec:transducerinfrastructure}

\subsection{Berkely Motes}

\subsection{MIT Plug (proprietary)}

\subsection{DIY w/ arduino/raspberry pi etc.}

\subsection{IEEE 1451 Compliant Smart Transducers}
\label{subsec:ieee1245}
IEEE 1451 is a family of Smart Transducer Interface Standards defining a set of open, common, network-independent communication interfaces for connecting transducers to microprocessors, instrumentation systems and control/field networks~\cite{lee:standard}. The main goal of IEEE 1451 is to allow network access to standardised transducer data through a common set of interfaces, whether the transducers are connected to systems or networks via wired or wireless means~\cite{lee:understanding}.

The definition of transducer remains the same as in previous sections, whilst a `smart' transducer extends the concept by integrating an analog or digital sensor or actuator element (a transducer) with a processing unit and a communication interface~\cite{elmenreich:smarttransducer}. A smart transducer implementation thus comprises a hardware or software device consisting of a small, compact unit containing a sensor or actuator element, a microcontroller, a communication controller and associated software from signal conditioning and calibration to diagnostics and communications~\cite{omg:smarttransducer}.

An IEEE 1451 compliant smart transducer additionally specifies a Transducer Electronic Data Sheet (TEDS) and partitions the system into two major components; a Network Capable Application Processor (NCAP) and a Transducer Interface Module (TIM), the two connected by a Transducer Independent Interface (TII).

The NCAP is a network node which performs application processing and network communication functions, while the TIM consists of transducer signal conditioning and data conversion and a number of sensors and actuators themselves (up to 255, in any combination). In this fashion many sensors and actuators can be hosted by a single TIM and many TIMs may communicate with a single NCAP via the TII which defines a communication medium and a protocol for transferring sensor information.

From a standards perspective this approach helps to achieve sensor `plug and play' and interoperability for industry and government~\cite{lee:understanding} whilst the distribution of measurement and control can dramatically reduce network load by allowing transducers themselves to determine whether observations are worthwhile communicating and also to encode them into formats more suited to the network being employed before transmitting~\cite{lee:distributed}.

This standardisation of communication with and between transducers will be extremely helpful for a standardised support framework for cross reality development. Instead of such a framework having to support numerous different transducer interfaces to cater for the numerous different hardware and software platforms on the market, all communication between framework and transducers can be achieved via IEEE 1451, allowing the framework to support any hardware/software platform that adheres to the standard.

OGC SWE (section \ref{sec:ogcswe}) adopts a similar approach by supporting interaction with IEEE 1451 compliant smart transducers through the standard interfaces presented by their implementations of the NCAP. This allows SWE to provide access to these transducers via interfaces implemented through open Web standards.

\textcolor{red}{\section{Virtual Environments}}
\label{sec:virtualenvironments}

\textcolor{red}{Talk here about how the definition of cross reality given by this paper doesn't require that the virtual environment component has to be a self-proclaimed virtual world (Second Life, OpenSimulator, Open Wonderland, etc.) but can be any package/program capable of presenting the user with a 3D virtual environment containing features such as an avatar or user-controllable camera/viewpoint, real-time updating of objects, etc.. So game engines, visualization engines, virtual reality engines, etc. are all suitable. Virtual worlds are/were prevalent in cross reality research however because of their ease of use, particularly for people (researchers) who do not have prior experience of 3D game/visualization engines, which often require knowledge and experience of external 3D modelling suites for content creation, which have steep learning curves.}

\subsection{Second Life/OpenSimulator}
For example MIT, my project.

\subsection{Sun OpenWonderland}
\subsection{3D Game/Visualisation Engines}
\begin{itemize}
	\item Unity (used by MIT's DoppelLab)
	\item CryEngine3
	\item Unigine, Blender Game Engine (BGE), ReaCtoR, etc.
\end{itemize}

\textcolor{red}{\section{Middleware}}
\label{sec:middleware}

\subsection{Open Geospatial Consortium $-$ Sensor Web Enablement}
\label{sec:ogcswe}
The Open Geospatial Consortium (OGC) is an international standards organisation comprising more than 400 industry, academic and government institutions collaborating in the development and implementation of open standards for geospatial and location services. The Sensor Web Enablement (SWE) project~\cite{botts:ogc} is a collection of such standards for exploiting Web-connected sensor systems.

SWE aims to enable all Web and/or Internet accessible sensors (including imaging devices such as surveillance cameras) to be accessible and controllable via the Web, through open Web standards, creating in effect a `sensor Web' that can be accessed in a similar manner as to how HTML and HTTP allow access to the WWW. As of 2008 SWE is in use by projects from Oak Ridge National Laboratory, the Defence Intelligence Agency, the European Space Agency and NASA, amongst others.

SWE is comprised of 7 individual standards, many adopted as OpenGIS standards, each concerned with a different aspect of Web-based interaction with sensor networks. Together they facilitate the discovery (with no \textit{a priori} knowledge) of sensor systems, including their properties, geospatial location and capabilities, the requesting, filtering and retrieving of observations from these systems, the encoding of these observations for communication across a network and the publishing of and subscription to alerts issued by sensors when certain conditions are met (such as particular environmental conditions).

SWE also harmonizes with other relevant sensor and alerting standards including IEEE 1451 (discussed in \ref{subsec:ieee1245}), the OASIS Common Alerting Protocol (an XML based data format for exchanging public warnings and emergencies between alerting technologies), the OASIS Asynchronous Service Access Protocol (standard descriptions of how to start, manage and monitor long running services) and Web Services Notification (specifications related to the WS-Resource framework that allow event driven programming between web services).

SWE was demonstrated in December 2006 at the fourth OGC Web Services testbed activity, OWS-4, using a hypothetical scenario of a bomb containing radioactive material detonating near New York City (this demonstration and others are available to view at ~\cite{ogc:demos}). SWE was used to find, access and integrate diverse geospatial resources from the vicinity of the detonation, many of which were live sensors, with all of the information flowing from different sources through Web services. First a radiation sensor triggered an alert which, via subscriptions, caused a chain of services to operate; other sensors in the vicinity were polled, surveillance cameras near the area became available to watch and position and NASA Earth Observation satellites were instructed to provide images of the area from orbit over the next few days so as to document the movement of the radioactive plume.

This was a demonstration of a Geospatial Decision Support System (GeoDSS) which allows a decision maker at a single workstation to identify and access geospatial resources no matter where they are, to bring them into an operational context and integrate them with other resources to support the overall decision making process. It is no stretch of the imagination to draw allusions between this example and a cross reality environment. For example a simulation of New York City in a virtual world could make use of SWE to collect observations from sensors and cameras spread about the city and instead of presenting these information to a decision maker via charts and graphs could use them to affect the state of the simulation in real time; observations from wind speed sensors could be reported to the virtual world's physics engine instead of being presented as arrows and numbers on a top-down map of the city, whilst commands to move a surveillance camera could instead be commands to open and close a door.

To labour the analogy further one could even imagine that cross reality could be of use to traditional GeoDSS, by presenting an alternative interpretation of the observations via a virtual world that the decision maker could navigate and even allowing aid workers to operate in the affected areas from the safety of a remote workstation, such as by unlocking and opening doors to allow trapped occupants to escape to safety.

As its name implies, SWE is primarily focussed on the provision of interfaces that exploit open Web standards to access and administer geospatially distributed \textit{sensors}. Whilst the standards provide methods for transmitting control information to these sensors, these are concerned with administration of sensors and are not intended to be used for controlling complex actuating devices. Whilst SWE can provide a cross reality developer with extensive infrastructure for implementing the augmented virtuality part of a cross reality system, it would need extending to provide sufficient infrastructure to allow the augmented reality part to be implemented successfully to the same degree.

\subsection{Ubiquitous Sensor Networks}
A Ubiquitous Sensor Network (USN) is a conceptual network built over existing physical networks which makes use of sensed data and provides knowledge services to anyone, anywhere and at any time, and where information is generated by using context awareness. USNs can utilize both traditional wired and wireless networks as well as Wireless Sensor Networks (WSN)~\cite{itu:y.2221}.

A WSN consists of spatially distributed sensor nodes that monitor physical and environmental conditions such as light intensity, temperature, humidity, sound levels, etc. and communicate these sensor readings back to a main location, usually via cooperative usage of short-range radios~\cite{baronti:wsn}.

Whilst initial WSN development focused on isolated networks that captured and transmitted sensed data to designated application systems, the continued development of the paradigm has led to the current ability to build an intelligent infrastructure of sensor networks connected to \textit{existing} physical networks. This information infrastructure has been dubbed Ubiquitous Sensor Networks, due to the introduction of sensors to pre-existing ubiquitous network infrastructure, and opens possibilities for applications and services based on sensor networks to various customers, from consumers and public organisations to enterprises and governments.

USN applications are therefore created by integrating sensor network applications and services into existing network infrastructure. This integration may in some cases require extensions and/or additions to core network architectures in order to cover the functional capability requirements extracted from USN applications and services. Thus the requirement for a USN middleware was recognised, to provide the necessary interface sets and functions for USN applications and for sensor networks to be easily integrated~\cite{kim:practical}.

The importance of this middleware is in the decoupling of USN application implementation and sensor network implementation, such that any physical sensor hardware from any vendor can be integrated with an existing network to produce an USN. This approach makes sensor networks a viable and cost effective endeavour, when manufacturers are able to sell the same sensor networks to different service providers to achieve different aims, rather than having to build bespoke platforms for different applications.

The importance of supporting USN applications and services in the Next Generation Network (NGN) environment has been recognised and addressed by the International Telecommunications Union (ITU) with the publishing of Rec.ITU-T Y.2221~\cite{itu:y.2221}. These recommendations cover administration and management of sensor networks, profile management, registration and discovery, quality of service (QoS) considerations, support for location-based service support, mobility support, security, and so forth.

Kim et al. recognised such a USN middleware as the key technology to propel the realization of the cross reality paradigm, by providing a solution to the inherent heterogeneity of sensor and RFID devices and the data that they produce. In their 2009 paper~\cite{kim:practical} they present an implementation of an USN middleware COSMOS that intermediated between various USN applications and simulated physical resources such as RFID reader networks and sensor and actuator networks, then showing how such a middleware functions as part of a larger cross reality platform.

COSMOS is intended as a software platform to provide development and running environments for various types of applications. COSMOS interacts with sensors via a Sensor Network Common Interface Processor, for which in depth details are not given, however conceivably this module could be replaced or extended to make use of IEEE 1451 to support as diverse a range of sensory platforms as possible and present COSMOS as a viable proof of concept for a standardised framework for cross reality development.

***mention XCREAM~\cite{park:xcream}?

\subsection{Microsoft SenseWeb}
SenseWeb from Microsoft Research is a system that allows peer production of sensing applications, producing new kinds of media and applications over existing data networks, by allowing users to grant access to their sensors to other remote users. The basic premise is that contributors deploy their own sensors or sensor networks, which might be designed for their own dedicated application or simply for sharing with other users, uploading the observations from these sensors to the SenseWeb system where they can then be accessed by other SenseWeb users through an application-specific GUI. Using SenseWeb, applications can initiate and access sensor data streams from shared sensors across the entire Internet~\cite{kansal:senseweb}.

The approach of the system is such that applications can access resources contributed by any entity in a uniform manner, regardless of the underlying platform of that entity. To achieve this, sensors are connected to a sensor gateway which provides a uniform interface to all components above it, hiding much of the complexity of the heterogeneous communication protocols, power capabilities, bandwidth capabilities, etc. of the numerous sensor platforms beneath.

However unlike with systems that implement IEEE 1451 compliant smart transducers, when a developer wishes to connect sensors to a sensor gateway using SenseWeb she must manually define many of the characteristics and properties of these sensors; type, location (longitude, latitude, altitude), description, etc. Additionally for the observations from different sensors to be made useful (eg two different types of temperature sensor, one that reports observations simply as a voltage from a thermistor and the second that performs some on-sensor processing and calibration and reports observations as a value in centigrade) a `data transformer' must be used to convert data semantics through processing. Whilst this approach to using intermediary data transformation allows for powerful handling of data, such as passing a video observation through a people count algorithm and only presenting to other users the output of this algorithm rather than the video itself, it adds complexity and manual implementation to accessing and making sense of observations which could have been achieved via a system such as the TEDS of IEEE 1451. When a user wishes to connect sensors to a sensor gateway, there is no automatic discovery by the sensor gateway of these sensors, nor of their characteristics (available sensors/actuators, calibration, type of communications interfaces, etc.)~\cite{microsoft:sensewebtutorial}.

Communication between sensors and sensor gateways relies upon the existence of drivers. Whilst many common sensing devies, including wireless motes and network cameras, already exist, this approach means that scalability to new and particularly unusual or unpopular (in terms of widespread adoption) sensing platforms is poor. Whilst SenseWeb provides some very powerful techniques at the application interface level for abstracting over heterogeneity at lower levels, such as differences in requirements to conserve power and bandwidth, by making extensive use of caching and distribution of tasking to different individual sensors when many are available that can achieve the same observation, all of this abstraction assumes that the sensors themselves have already been integrated with the system, usually requiring manual human intervention.

SenseWeb presents a very polished interface to the end user, making it extremely easy and intuitive to pull in observations from sensory resources around the globe and put them to good use, however this ease of interaction at the end user level comes at a price lower down where sensor attributes must sometimes be manually specified and intermediary processing must sometimes be implemented. SenseWeb serves as a good example of what the interface to a standardised framework for cross reality development should be like, however automatic resource discovery and administration at the lower levels would make the system more desirable to developers at all levels, rather than just at the receiving end of observations.

\subsection{Global Sensor Network (GSN)}
GSN is a middleware developed by \'{E}cole Polytechnique F\'{e}d\'{e}rale de Lausanne (EPFL) to support the rapid and simple deployment of a wide range of sensor network topologies, facilitate the flexible integration and discovery of sensor networks and sensor data, enable fast deployment and addition of new platforms, provide distributed querying, filtering and combination of sensor data and support dynamic adaptation of the system configuration during operation~\cite{aberer:gsn}.

EPFL researchers realised that as the price of wireless sensors decreased and the number of autonomous sensor networks increased, the emerging desire for a `Sensor Internet' was being hampered by the difficulty of sharing and integrating data among heterogeneous sensor networks, as previous research in the sensor network domain had mainly focused on routing, data aggregation and energy conservation inside a single sensor network.

After identifying the standard strategy adopted successfully in other domains of addressing such a problem by devising a middleware which provides powerful abstractions allowing easy integration of concrete physical platforms, EPFL set about developing GSN.

GSN's key abstraction is the `virtual sensor', which abstract from implementation details of access to sensor data and corresponds either to a data stream received directly from a sensor or to a data stream derived from other virtual sensors. Virtual sensors can be any kind of data producer; actual sensors, wireless cameras, desktop computers, mobile phones, or even a combination of these. Each virtual sensor can have any number of input streams but produces exactly one output stream based on these input streams and some logical processing.

Virtual sensors are described in a declarative deployment descriptor, which is an XML file defining various attributes; a unique identifier, any number of input streams, the structure of the output stream, etc. Defining this XML file is all that is required to deploy a new sensor as long as GSN includes software support for the concerned hardware/software. This support is provided by wrappers conforming to the GSN API and several such wrappers are provided to cover common hardware/software including a generic HTTP wrapper for accessing data from devices using HTTP GET and POST, and a serial forwarder wrapper that supports interaction with TinyOS WSN motes over a direct serial connection. Implementing new wrappers is not a substantial endeavour either, as even complex wrappers are only a few hundred lines of code (mostly Java).

GSN has been demonstrated on real hardware through experiments with typical high-load application profiles and is used as the primary streaming data database for the ETH Centre for Competence Environment and Sustainability (CCES) and through the Swiss Experiment platform project the usage of GSN is increasing, currently in the region of 80 $-$ 100 million data points.

GSN is also open source under the GNU GPL available freely on the Web~\cite{gsn:url}, making it ideal to take and extend for cross reality middleware.

\textcolor{red}{summary of how it could/can/has (be(en)) used for cross reality}

\subsection{MPEG-V}
The currently under development MPEG$-$V standard, `Information Exchange with Virtual Worlds', aims to address this shortcoming, by defining standards for information and media exchange between virtual worlds and interfaces between virtual worlds and the real world.

\begin{quotation}
	\textit{``The `Information exchange with Virtual Worlds' project intends to provide a standardized global framework and associated interfaces, intermediate formats definitions and the like, to enable the interoperability between virtual worlds (as for example Active Worlds, Second Life, IMVU, Google Earth, Virtual Earth and many others) and between virtual worlds and the real world (sensors, actuators, vision and rendering, robotics (e.g. for revalidation), (support for) independent living, social and welfare systems, banking, insurance, travel, real estate, rights management and many others).''} \\ \textcolor{red}{cite (Summary of MPEG-V, International Organization for Standardization)}
\end{quotation}

The standard is subdivided into 6 parts, of which particular interest for cross reality developers are part 2, Control Information, and part 3, Sensory Information. Part 2 defines interaction using XML for controlling a myriad of physical and environmental properties from simple concepts such as `Light capability type' to more advanced concepts such as `Fog capability type'. Likewise part 3 defines XML-based interaction with sensing primitives from `Temperature effect' to `Passive Kinesthetic Motion effect'.

\begin{figure}[h!tbp]
\centering
\includegraphics[width=0.45\textwidth]{mpegv.png}
\caption{Example instantiation of the proposed MPEG$-$V architecture, presenting the mapping for bidirectional exchange of information between real and virtual worlds.}
\label{mpegv}
\end{figure}

\textcolor{red}{explain difference between basic actuators and sensory effect devices}

\textcolor{red}{ISO/IEC 23005 has now been released at version 1!}
\st{Although MPEG$-$V is currently under development and has yet to be ratified}, it is a promising endeavour that when complete should greatly aid not just the development of cross reality systems in research, but also the use of mixed reality in entertainment ranging from video games to next generation cinema.
