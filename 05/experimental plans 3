> Accuracy of position tracking (IndoorAtlas)

How do we measure 'accuracy' of IndoorAtlas? Because every position that it gives us will always be exactly on one of the paths that we created, so if the user is following the path their IndoorAtlas position will always be on the path.

Can we measure how far along the path IndoorAtlas thinks we are than where we really are? Eg. 'for this position IndoorAtlas thinks we are 2.3m further along this path than we really are'.

Can we measure how long it takes for position to 'catch up', in a number of different scenarios? Eg from a cold fix it takes <x> seconds to stabilise, from a stationary hot fix it takes <y> seconds to realise that we are moving & then <z> seconds to catch up.

Or is qualitative evaluation better? Asking users how well they think/experience IndoorAtlas tracking them when walking different routes, performing certain tasks, etc.?

> Accuracy of head tracking (Occulus)

Can this be measured quantitatively? Are we concerned with gyro drift over time? Remember that in the actual Mirrorshades scenario the inaccuracy of the positional tracking will mean that the inaccuracy of the head tracking will probably be moot.

Accuracy of head tracking in a switching RW/VR fashion could be measured through experimentation; have a person stand stationary somewhere with markings/targets on the wall & a VR model of the same & then ask them to look at different targets & then switch & report on how much the real & virtual targets differ in their perceived position (with time as well, eg as the experiment continues, does the gyro drift cause the perceived differences to increase).

> Performance of VR graphics

This can simply be logged during actual Mirrorshades experiments.

> Performance/fidelity of RW camera feed

This will be harder to measure, as it is largely qualitative(?).

> Experience/worth of switching

This is completely qualitative.

Igroup Presence Questionnaire suitability;

INV1, INV2 & INV3 are directly relevant, though the framing is wrong - high sense of presence in VR is supposed to elicit;

INV1 - low
INV2 - high
INV3 - low
INV4 - high

But for Mirrorshades we would want something more like;

INV1 - medium/high
INV2 - low
INV3 - medium/high
INV4 - low/medium

'Virtual Environments and the Sense of Being There: An SEM Model of Presence' says (p7) that SP3 & INV2 seem to sit at the two ends of a continuum, that one will always be high & the other low when using the IPQ to measure presence in a VR (low SP3/high INV2 for high sense of presence in VR, high SP3/low INV2 for low sense of presence in VR). However if using the IPQ for a cross reality application, one would expect to have low/medium SP3 & low INV2.

*** But what is the baseline that I am comparing it to? Static positions at which the user can pick up a tablet/phone to look around? Or static positions at which they can don a HMD to look around?

Comparing static tablet/phone with mobile HMD wouldn't be good, as then the dominant variable is the screen rather than the mobility. Witmer & Singer 'Measuring Presence' p227 "If users perceive that a they are outside of the simulated environment and looking in (e.g., whilst viewing the environment via a CRT display), the immersive aspect is lost..."

So we want to compare the *same* display mechanism both static & mobile